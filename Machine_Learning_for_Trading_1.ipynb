{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning for Trading 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpxH0Z59mt0rWd7gIUe95h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpetrovic89/Machine-Learning-for-Trading-Course-Notes/blob/main/Machine_Learning_for_Trading_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfvmmneDUkW"
      },
      "source": [
        "# Reading and Ploting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGCOmO8qDJzx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Uj_mbr74Oq"
      },
      "source": [
        "Bollinger Bands\n",
        "https://classroom.udacity.com/courses/ud501/lessons/4156938722/concepts/41444292690923"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ3QR_G_78Mq"
      },
      "source": [
        "Daily Returns \n",
        "https://classroom.udacity.com/courses/ud501/lessons/4156938722/concepts/41444292700923"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ror7ul6dBPwG"
      },
      "source": [
        "Cumulative returns\n",
        "https://classroom.udacity.com/courses/ud501/lessons/4156938722/concepts/41444292710923"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsefirEcBl7G"
      },
      "source": [
        "Histograms and ScatterPlot\n",
        "https://classroom.udacity.com/courses/ud501/lessons/4179049354/concepts/42379985630923\n",
        "\n",
        "Real world use of kurtosis\n",
        "https://classroom.udacity.com/courses/ud501/lessons/4179049354/concepts/46681086960923"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKZ9L0hyB3Pj"
      },
      "source": [
        "# Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIlUtH2JDRnM"
      },
      "source": [
        " Sharpe Ratio https://classroom.udacity.com/courses/ud501/lessons/4242038556/concepts/41998985450923"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOSuC64cE1Ma"
      },
      "source": [
        "# Optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0kjksP3E7JR"
      },
      "source": [
        "Find Minimal values in functions \n",
        "\n",
        "Build paramized models based on data \n",
        "\n",
        "Refine alocations to stocks in portfolio\n",
        "\n",
        "**How to use Optimizers?**\n",
        "\n",
        "    1. Provide function to minimize\n",
        "    2. Provide initial guess\n",
        "    3. Call the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfy9UBmNGXC2"
      },
      "source": [
        "# 2. Headge Funds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kox9TeHdGjIp"
      },
      "source": [
        "Intrinsic value\n",
        "https://classroom.udacity.com/courses/ud501/lessons/4389588610/concepts/44320329420923"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVNbicB5Gtt7"
      },
      "source": [
        "Interest Rate and Discount Rate are terms that refer to essentially the same quantity, but are used to distinguish two slightly different use cases:\n",
        "\n",
        "\n",
        "**Interest Rate** is used with a given Present Value, to figure out what the Future Value would be.\n",
        "\n",
        "**Discount Rate** is used when we have a known or desired Future Value, and want to compute the corresponding Present Value.\n",
        "\n",
        "\n",
        "For instance, in this case we want to sum up all future dividends - equal to a constant ($1 or FV) every year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ibTwNQCHZ45"
      },
      "source": [
        "The Capital Asset Pricing Model (CAPM)\n",
        "\n",
        "**CAPM Equation**  Vazno\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCyMgbJgD82o"
      },
      "source": [
        "CAPM vs active management\n",
        "\n",
        " Passive Buy index and hold\n",
        "\n",
        " Active: pick stocks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I9xoNMcGfwP"
      },
      "source": [
        "Arbitrage Pricing Theory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWQ65XQYGiVL"
      },
      "source": [
        "## Risk for Headge Funds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHPDA7V3G1NX"
      },
      "source": [
        "CAPM for HEADGE FUNDS summary\n",
        "\n",
        "**Assuming** \n",
        "\n",
        "Information -> alpha\n",
        " \n",
        "Beta\n",
        "\n",
        "**CAPM enebles**\n",
        "\n",
        "minimize market risk BETA portfolio = 0\n",
        "\n",
        "Wi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAjKB6OlN2vu"
      },
      "source": [
        "## Technical Analsis\n",
        "\n",
        "Invidual indicators weak \n",
        "\n",
        "Combinations stronger\n",
        "\n",
        "Look for contrast (stock vs market)\n",
        "\n",
        "Shorter time periods\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-23iiavVDuum"
      },
      "source": [
        "A few indicators: Momentum\n",
        "\n",
        "A few indicators: Simple moving average\n",
        "\n",
        "A few indicators: Bollinger Bands\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn0U06nTIQhA"
      },
      "source": [
        "survivor bias free dats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euNWzrnhNECg"
      },
      "source": [
        "## Grinold's Fundamental Law\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PIC_YZg0nf"
      },
      "source": [
        "Information Ratio \n",
        "\n",
        "Information Coefficient - correlation of forecast to returns\n",
        "\n",
        "Breadth - number of trading opportunities per year\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMFTRuQQaIgl"
      },
      "source": [
        "**The Fundamental Law**\n",
        "\n",
        "IR = IC * SquaredRoot(Breadth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTurvEvCei2A"
      },
      "source": [
        "## Portfolio Optimization and Efficient frontier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAS6KtaUepm8"
      },
      "source": [
        "What is RISK? \n",
        "\n",
        "STandard Deviation of daily returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU8PaCP9hylY"
      },
      "source": [
        "MVO\n",
        "\n",
        "Mean Variance Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmtBwfFpj8Yi"
      },
      "source": [
        "# 3. Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TieZ83NkhFFG"
      },
      "source": [
        "Roll forward cross validation\n",
        "\n",
        "Bagging\n",
        "\n",
        "Boosting\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZBoJ2tSc_tI"
      },
      "source": [
        "Reinforcment Learning\n",
        "\n",
        "State\n",
        "\n",
        "Policy\n",
        "\n",
        "Action\n",
        "\n",
        "T\n",
        "\n",
        "Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dv_A2oAf1MA"
      },
      "source": [
        "Markov Decisions Problem\n",
        "\n",
        "Set of states S\n",
        "\n",
        "Set of actions A\n",
        "\n",
        "Transition function T[s,a,s']\n",
        "\n",
        "Revard function R[s,a]\n",
        "\n",
        "Find polici P(s) that will maximize reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDBelHT1hB1b"
      },
      "source": [
        "Unknown transitions and rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNh7KLr4aHaQ"
      },
      "source": [
        "## Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zFdBxwaNr1"
      },
      "source": [
        "**What is Q?**\n",
        "\n",
        "Q[s,a] = immediate reward + discounted reward\n",
        "\n",
        "How to use Q?\n",
        "\n",
        "Policy\n",
        "P(s) = argmax-a(Q[s,a])\n",
        "\n",
        "Optimal Policy\n",
        "P*(s)\n",
        "\n",
        "Optimal Q-table\n",
        "Q*[s,a]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQrx-gbhTnz"
      },
      "source": [
        "Q Learning Procedure\n",
        "\n",
        "    select trainning data\n",
        "\n",
        "    iterate over time <s,a,s',r>\n",
        "\n",
        "    test policy P\n",
        "\n",
        "    repeat until coverage\n",
        "\n",
        "Details in iterate over time\n",
        "    set starttime, init Q[]\n",
        "\n",
        "    compute s\n",
        "    \n",
        "    select a\n",
        "    \n",
        "    observe r,s'\n",
        "    \n",
        "    update Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwC2XWGmYoMG"
      },
      "source": [
        "**Update Rule**\n",
        "\n",
        "alpha learning rate 0 to 1.0 (usually 0.2)\n",
        "\n",
        "gamma discount rate 0 to 1.0\n",
        "\n",
        "Q'[s,a] = (1-alpha)Q[s,a] + alpha * improved estimate\n",
        "\n",
        "**Q'[s,a] = (1-alpha)Q[s,a] + alpha(r + gamma * Q[s', argmax-a'(Q[s',a'])])**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFfzB_s5cvmS"
      },
      "source": [
        "Two finner points\n",
        "\n",
        "Success comes from exploration\n",
        "\n",
        "Choose random action with probability c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pd_9ZWzd5_Y"
      },
      "source": [
        "The Trading Problem: Actions\n",
        "\n",
        "      Buy\n",
        "\n",
        "      Sell\n",
        "\n",
        "      Do Nothing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJU6eZ6-ekgB"
      },
      "source": [
        "The Trading Problem: Rewards\n",
        "\n",
        "daily return \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydbBHlSmfYFl"
      },
      "source": [
        "The Trading Problem: State\n",
        "\n",
        "Creating the State\n",
        "\n",
        "    state is a integer\n",
        "\n",
        "    discretize each factor \n",
        "\n",
        "    combine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6LoMDtk38F"
      },
      "source": [
        "Discretizing \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "stepsize = size(data)/steps\n",
        "data.sort()\n",
        "for i in range(0, steps):\n",
        "    threshold[i] = data[(i + 1) * stepsize]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjpldopmZYsW"
      },
      "source": [
        "Q-Learning Recap\n",
        "\n",
        "Building a model\n",
        "\n",
        "*   define states(features), actions(buy,sell,hold), rewards(daily returns)\n",
        "*   choose in-sample K training periods\n",
        "*   iterate: Q Table update\n",
        "*   backtest\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJF4Sex0gsIo"
      },
      "source": [
        "## Dyna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0C0X_agwfT"
      },
      "source": [
        "Dyna-Q Big Picture\n",
        "\n",
        "Q-Learning\n",
        "\n",
        "    init Q table\n",
        "\n",
        "    observe s\n",
        "\n",
        "    execute a, observe s',r\n",
        "\n",
        "    update Q with <s,a,s',r>\n",
        "\n",
        "Dyna-Q\n",
        "\n",
        "    Learn Model T'[s,a,s'] = Trasiction Matrix,   R'[s,a] = Reward Function\n",
        "\n",
        "    Halucinate experiance\n",
        "\n",
        "    update Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECoDdczli6wh"
      },
      "source": [
        "Learning T\n",
        "\n",
        "\n",
        "\n",
        "init Tc[] = 0.00001\n",
        "\n",
        "while executing, observe s,a,s'\n",
        "\n",
        "increment  Tc[s,a,s']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ9lt5praV4I"
      },
      "source": [
        "Learning R\n",
        "\n",
        "R[s,a] expected reward for s,a \n",
        "\n",
        "r      immediate reward\n",
        "\n",
        "R'[s,a] = (1- alpha) R[s,a] + alpha * r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXBVDYbIeJ4W"
      },
      "source": [
        "# Quantative strategy \n",
        "\n",
        "1. Look for theoretical sound ideas (fundamental principals of financial markets and economics)\n",
        "\n",
        "2. Empirically  tested \n",
        "\n",
        "3. Be scared of complexity\n",
        "\n",
        "Don't iterate on same test data (7-8 times)"
      ]
    }
  ]
}